{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import socket\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ANACONDA\\lib\\site-packages\\numba\\core\\decorators.py:150: NumbaDeprecationWarning: \u001b[1mThe 'target' keyword argument is deprecated.\u001b[0m\n",
      "  warnings.warn(\"The 'target' keyword argument is deprecated.\", NumbaDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "@jit(forceobj=True,target='cpu',looplift=True)\n",
    "def similarity(x):\n",
    "    \"\"\" 计算余弦相似度 \"\"\"\n",
    "    x=np.array(x)\n",
    "    x=normalize(x) #单位化\n",
    "    res=np.dot(x , x.T)\n",
    "    return res\n",
    "\n",
    "def calc_prin_eigv(mat, eps):\n",
    "    \"\"\"对于给定的矩阵mat，迭代计算其主特征向量\"\"\"\n",
    "    x = np.random.rand(mat.shape[1])\n",
    "    while True:\n",
    "        x1 = np.dot(mat, x)\n",
    "        x1 = x1 / np.linalg.norm(x1)\n",
    "        if np.abs(x1-x).max() < eps:\n",
    "            break\n",
    "        x = x1\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class LocalServer(object):\n",
    "    def __init__(self, host, port):\n",
    "        \"\"\"\n",
    "\n",
    "        初始化，包括：\n",
    "        1.读取和预处理数据，构建词典\n",
    "        2.计算TF-IDF矩阵\n",
    "        3.计算相似词（或者从已保存好的synonym.txt中读取）\n",
    "\n",
    "        \"\"\"\n",
    "        self.address = (host, port)\n",
    "        self.data=pd.read_csv('./data/all_news.csv')\n",
    "        print(len(self.data))\n",
    "\n",
    "        with open('./data/english', 'r', encoding='utf-8') as f:\n",
    "            self.stop_words = f.readlines()  # 读取全部内容后，按行存储为list\n",
    "        self.stop_words = set(i.strip('\\n') for i in self.stop_words)\n",
    "\n",
    "        processed = self.data[\"body\"].apply(self.process) #去除标点，停用词和低频词\n",
    "        \n",
    "        self.data['processed']=processed\n",
    "        self.vocab=self.build_words_Dictionary() #词典\n",
    "        #print(self.word_dic)\n",
    "\n",
    "        self.IDF_dict=self.IDF() #词典中的所有词的IDF，构成字典{ word1:IDF1, word2:IDF2, ...}的形式\n",
    "        vec=self.TF_IDF_vec() #计算文章向量（TF-IDF矩阵），其中的值其实就是TF-IDF值\n",
    "        self.data['vec']=pd.Series([x for x in vec]) #文章向量\n",
    "\n",
    "        self.word_similarity() #从已保存的文件中获取词相似度，以做模糊匹配\n",
    "        #print(self.words_similarity)\n",
    "\n",
    "        #底下这些代码是将上面的vec取转置，然后再求相似度。\n",
    "        #vec的每一行代表文章，每一列代表词，因此取转置再求相似度就是求词之间的相似度\n",
    "        #我只运行了其一次，并将结果存到了synonym.txt中\n",
    "        #synonym.txt中存的是每个词，与其最相似的前三个词（包括它自己，所以它自己就会是第一个）\n",
    "        #之后要做模糊匹配，就从synonym.txt中读取即可(当然以下代码还是可运行的)\n",
    "        '''print(\"type(vec)==\",type(vec))\n",
    "        tmp=np.array([list(x) for x in vec]).T\n",
    "        print(tmp.shape)\n",
    "        tmp=[list(x) for x in tmp]\n",
    "        #print(tmp)\n",
    "        \n",
    "        word_sim=similarity(tmp)\n",
    "        \n",
    "        self.word_sim=word_sim\n",
    "        self.sim_word_of_each_word=[]'''\n",
    "        \n",
    "        \n",
    "\n",
    "        '''wfile=open('./synonym.txt','w')\n",
    "        \n",
    "        for i,sim in enumerate(word_sim):\n",
    "            loc=np.argsort(-sim)[:3]\n",
    "            now_word=self.vocab[i]\n",
    "            sim_word=np.array(self.vocab)[loc] #取前三个最相似的\n",
    "            print(f'{now_word} : {sim_word}',file=wfile)\n",
    "            self.sim_word_of_each_word.append(sim_word)\n",
    "        wfile.close()\n",
    "        self.word_similarity() #从已保存的文件中获取词相似度，以做模糊匹配\n",
    "        '''\n",
    "        #本地服务器初始化完成\n",
    "        print(\"Local Server has completed initialization!\")\n",
    "        \n",
    "\n",
    "    def word_similarity(self):\n",
    "        \"\"\"直接从synonym.txt中读取相似词，每个词有三个与它最近似的词\"\"\"\n",
    "        rfile=open('./synonym.txt','r')\n",
    "        content=rfile.readlines()\n",
    "        rfile.close()\n",
    "        self.words_similarity={} #原词与其相似词们构成的字典\n",
    "        for s in content:\n",
    "            s=s.strip('\\n')\n",
    "            ls=s.split(':')\n",
    "            word=ls[0].strip(' ') #原词\n",
    "            ls[1]=ls[1].strip(' ')[1:-1]\n",
    "            ls[1]=ls[1].split(' ')\n",
    "            sim_words=[eval(x) for x in ls[1]] #相似词构成的列表\n",
    "            self.words_similarity[word]=sim_words \n",
    "        return \n",
    "        \n",
    "    def process(self,x): \n",
    "        \"\"\"处理正文，去除标点，停用词和低频词\"\"\"\n",
    "        MIN=2 #将出现次数小于MIN的词都视作低频词\n",
    "        x=re.sub('[^A-Za-z]+', ' ', x).lower()\n",
    "        x = x.split(' ')\n",
    "        ls = [z for z in x if z not in self.stop_words]\n",
    "        cnt=Counter(ls)\n",
    "        res = [z for z in ls if cnt[z]>=MIN]\n",
    "        return res\n",
    "    \n",
    "    def build_words_Dictionary(self): \n",
    "        \"\"\"构建词典(self.vocab)\"\"\"\n",
    "        vocab=set()\n",
    "        for pro in self.data['processed']:\n",
    "            vocab|=set(pro) \n",
    "        vocab=sorted(list(vocab))\n",
    "\n",
    "        wfile=open('./data/vocab.txt','w')\n",
    "        for word in vocab:\n",
    "            print(word,file=wfile)\n",
    "        wfile.close()\n",
    "        return vocab\n",
    "    \n",
    "    def IDF(self):\n",
    "        \"\"\"计算词典中每个词的IDF值,构成字典{ word1:IDF1, word2:IDF2, ...}\"\"\"\n",
    "        res={} #初始化空字典\n",
    "        Y=len(self.data)\n",
    "        for word in tqdm(self.vocab,desc='IDF'):\n",
    "            cnt=1\n",
    "            for processed_data in self.data['processed']:\n",
    "                if word in processed_data:\n",
    "                    cnt+=1\n",
    "            res[word]=math.log(Y/cnt)\n",
    "        return res\n",
    "\n",
    "    def TF_IDF_vec(self):\n",
    "        \"\"\"计算TF-IDF矩阵\"\"\"\n",
    "        res=np.zeros((len(self.data),len(self.vocab))) #初始化零矩阵\n",
    "        for i,data in enumerate(tqdm(self.data['processed'],desc='TF-IDF')):\n",
    "            cnt=Counter(data) #其实就是TF，不过这里的TF还没有除以文章长度，到后面再除\n",
    "            N=len(data) #文章的长度\n",
    "            for word in data: #计算每个出现在文章中的词的TF-IDF值\n",
    "                loc=self.vocab.index(word) #文章向量中，每个词的下标取决于其在词典(self.vocab)中的下标\n",
    "                tmp=self.IDF_dict[word]*cnt[word]/N\n",
    "                res[i][loc]=tmp\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        在服务器端实现合理的并发处理方案，使得服务器端能够处理多个客户端发来的请求\n",
    "        \"\"\"\n",
    "        try:\n",
    "            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "            server.bind(self.address)\n",
    "            server.listen(5)\n",
    "        except socket.error as msg:\n",
    "            print(msg)\n",
    "            sys.exit(1)\n",
    "        print('Waiting connection...')\n",
    "\n",
    "        while 1:\n",
    "            conn,addr=server.accept()\n",
    "            print(f'{conn}connected!')\n",
    "            t=Thread(target=self.TextSearch,args=(conn,addr))\n",
    "            t.start()\n",
    "\n",
    "    def TextSearch(self,conn,addr):\n",
    "        \"\"\"\n",
    "        实现文本检索，以及服务器端与客户端之间的通信\n",
    "        \n",
    "        1. 接受客户端传递的数据， 例如检索词\n",
    "        2. 调用检索函数，根据检索词完成检索\n",
    "        3. 将检索结果发送给客户端\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        conn.send((\"Connected!\").encode())\n",
    "        text=conn.recv(1024).decode()\n",
    "        \n",
    "        words_list=text.split(' ') #检索词列表\n",
    "        useful_data=np.array(self.data[['title','body','vec','id']])\n",
    "        res=[] #检索到的内容\n",
    "        all_title=[] #记录全部标题，从而不会将相同标题的文章两次加到检索内容中\n",
    "        all_vec=[] #所有检索到的文章的原始文章向量（也就是没PCA降维过的）\n",
    "        new_words_list=[] #新的检索词列表，其实就是把检索词中，不存在于词典中的去掉，以及将原词的相似词加了进来，\n",
    "        for word in words_list:\n",
    "            word=word.lower()\n",
    "            if word not in self.vocab:\n",
    "                continue\n",
    "            new_words_list.extend(self.words_similarity[word]) #原词也总是包含于其相似词列表中，所以不用额外再把原词加到列表中\n",
    "\n",
    "        for word in new_words_list: #开始检索\n",
    "            loc=self.vocab.index(word) \n",
    "            for data in useful_data:\n",
    "                tmp=data[2][loc] #该检索词在当前这篇文章中的TF-IDF值\n",
    "                if data[0] not in all_title and (tmp>0.1 or word in data[0].lower()): \n",
    "                    res.append([data[0],data[1],tmp]) #标题，正文，TF-IDF 构成的元组\n",
    "                    all_title.append(data[0])\n",
    "                    all_vec.append(data[2])\n",
    "        \n",
    "        if all_vec!=[]: #检索到了一些文章\n",
    "\n",
    "            #计算检索到的文章之间的相似度，再迭代计算主特征向量，\n",
    "            # 然后将 其 与 每篇文章与其他文章的相似度向量 的余弦相似度作为排序依据（HITS算法）\n",
    "            useful_similirity=similarity(all_vec) \n",
    "            prin_eigv = calc_prin_eigv(useful_similirity, 1e-6) \n",
    "            for i,vec in enumerate(useful_similirity):\n",
    "                res[i][2]=np.dot(prin_eigv,vec) #res[i][2]对应的是TF-IDF值，已经无用了，所以不妨覆盖掉它\n",
    "\n",
    "            res=sorted(res,key=lambda x:x[2],reverse=True) #降序排序\n",
    "            res=np.array(res,dtype=object) #将res先变成array，只取res的前两列（标题和正文），然后再返回\n",
    "            res=res[:,:2]\n",
    "            res=[tuple(x) for x in res]\n",
    "            \n",
    "            conn.send((repr(res)).encode())\n",
    "            conn.close()\n",
    "\n",
    "        else: #什么也没检索到，返回空的列表\n",
    "            conn.send(('[]').encode())\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IDF: 100%|██████████| 10131/10131 [00:43<00:00, 232.33it/s]\n",
      "TF-IDF: 100%|██████████| 2225/2225 [00:18<00:00, 119.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(vec)== <class 'numpy.ndarray'>\n",
      "(10131, 2225)\n",
      "Local Server has completed initialization!\n"
     ]
    }
   ],
   "source": [
    "server = LocalServer('127.0.0.1', 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文章聚类与检索结果评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (2225, 10131)\n"
     ]
    }
   ],
   "source": [
    "TF_IDF=normalize(np.array([x for x in server.data['vec']]))\n",
    "print(TF_IDF,TF_IDF.shape)\n",
    "\n",
    "km = KMeans(n_clusters=5).fit(TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取原分类 Y\n",
    "dict = {'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}\n",
    "Y = np.array([dict[x] for x in server.data['topic']])\n",
    "\n",
    "\n",
    "def Purity(original, now_label):\n",
    "    \"\"\"计算purity\"\"\"\n",
    "    n_ir = np.zeros((len(np.unique(original)), len(np.unique(now_label))))# 属于预定义类i且被分到第r个聚类的文档个数\n",
    "    \n",
    "    n_r = np.zeros((len(np.unique(original)),))# 第r个聚类类别的文档个数\n",
    "    \n",
    "    for ori, now in zip(original.reshape(now_label.shape), now_label):\n",
    "        n_ir[ori, now] += 1\n",
    "        n_r[now] += 1\n",
    "\n",
    "    P = np.amax(n_ir, axis=0) / n_r\n",
    "    purity = (n_r / len(now_label) * P).sum()\n",
    "    return purity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7015730337078652\n"
     ]
    }
   ],
   "source": [
    "print(Purity(Y,km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "purity达到了0.7，看上去效果还是不错的。\n",
    "通过实验，\n",
    "1.当将MIN设为3时（也就是将文章中出现次数少于3次的词视作低频词），词典的大小大约为6000词，此时purity只有0.5左右\n",
    "2.将MIN设为2时（也就是现在这种情况），词典大小约为10000词，此时purity有0.7左右\n",
    "3.所以可以猜测，当MIN设为1时，purity会更高，这意味着通过TF-IDF得到的文章向量是合理的，purity不够高只是因为词典的大小不够大。\n",
    "（MIN为1的时候，估计要算好一会，所以我没做MIN=1的实验了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行服务器端\n",
    "启动服务器之后，在run.ipynb中运行客户端图形界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting connection...\n",
      "<socket.socket fd=4900, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 1234), raddr=('127.0.0.1', 57084)>connected!\n"
     ]
    }
   ],
   "source": [
    "#server = LocalServer('127.0.0.1', 1234)\n",
    "server.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98eabb4ecb079f6ab10f8dee230a66b2a15b8304706421bed2d37b91eaa48f37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
