{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import socket\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MeanShift\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\numba\\core\\decorators.py:150: NumbaDeprecationWarning: \u001b[1mThe 'target' keyword argument is deprecated.\u001b[0m\n",
      "  warnings.warn(\"The 'target' keyword argument is deprecated.\", NumbaDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "@jit(forceobj=True,target='cpu',looplift=True)\n",
    "def similarity(x): #余弦相似度\n",
    "    print(len(x[0]))\n",
    "    N=len(x)\n",
    "    res=np.zeros((N,N))\n",
    "    norm=[]\n",
    "    for v in x:\n",
    "        norm.append(math.sqrt(np.dot(v,v)))\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        for j in range(i,N):\n",
    "            tmp=norm[i]*norm[j]\n",
    "            if tmp==0:\n",
    "                res[i][j]=0\n",
    "                res[j][i]=0\n",
    "            else:\n",
    "                res[i][j]=np.dot(x[i],x[j])/tmp\n",
    "                res[j][i]=res[i][j]\n",
    "    return res\n",
    "\n",
    "class LocalServer(object):\n",
    "    def __init__(self, host, port):\n",
    "        self.address = (host, port)\n",
    "        self.data=pd.read_csv('./data/all_news.csv')\n",
    "        print(len(self.data))\n",
    "\n",
    "        with open('./data/english', 'r', encoding='utf-8') as f:\n",
    "            self.stop_words = f.readlines()  # 读取全部内容后，按行存储为list\n",
    "        self.stop_words = set(i.strip('\\n') for i in self.stop_words)\n",
    "\n",
    "        processed = self.data[\"body\"].apply(self.process) #去除标点，停用词和低频词\n",
    "        \n",
    "        self.data['processed']=processed\n",
    "        self.vocab=self.build_words_Dictionary() #词典\n",
    "        #print(self.word_dic)\n",
    "\n",
    "        self.IDF_dict=self.IDF() #词典中的所有词的IDF，构成字典{ word1:IDF1, word2:IDF2, ...}的形式\n",
    "        vec=self.data['processed'].apply(self.TF_IDF_vec) #计算文章向量，其中的值其实就是TF-IDF值\n",
    "        self.data['vec']=vec \n",
    "\n",
    "        self.word_similarity() #从已保存的文件中获取词相似度，以做模糊匹配\n",
    "        #print(self.words_similarity)\n",
    "\n",
    "        #底下这些代码是将上面的vec取转置，然后再求相似度。\n",
    "        #vec的每一行代表文章，每一列代表词，因此取转置再求相似度就是求词之间的相似度\n",
    "        #由于以下代码运行要很久（20min左右），所以我只运行了其一次，并将结果存到了synonym.txt中\n",
    "        #synonym.txt中存的是每个词，与其最相似的前三个词（包括它自己，所以它自己就会是第一个）\n",
    "        #之后要做模糊匹配，就从synonym.txt中读取即可\n",
    "        \n",
    "        '''print(\"type(vec)==\",type(vec))\n",
    "        tmp=np.array([list(x) for x in vec]).T\n",
    "        print(tmp.shape)\n",
    "        tmp=[list(x) for x in tmp]\n",
    "        #print(tmp)\n",
    "        \n",
    "        word_similarity=similarity(tmp)\n",
    "        \n",
    "        self.word_similarity=word_similarity\n",
    "        self.sim_word_of_each_word=[]'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        wfile=open('./synonym.txt','w')\n",
    "        \n",
    "        for i,sim in enumerate(word_similarity):\n",
    "            loc=np.argsort(-sim)[:3]\n",
    "            now_word=self.vocab[i]\n",
    "            sim_word=np.array(self.vocab)[loc] #取前三个最相似的\n",
    "            print(f'{now_word}:{sim_word}',file=wfile)\n",
    "            self.sim_word_of_each_word.append(sim_word)\n",
    "        wfile.close()\n",
    "        '''\n",
    "\n",
    "        #print(self.data)\n",
    "\n",
    "    def word_similarity(self):\n",
    "        \"\"\"直接从synonym.txt中读取，每个词有三个与它最近似的词\"\"\"\n",
    "        rfile=open('./synonym.txt','r')\n",
    "        content=rfile.readlines()\n",
    "        rfile.close()\n",
    "        self.words_similarity={} #原词与其相似词们构成的字典\n",
    "        for s in content:\n",
    "            s=s.strip('\\n')\n",
    "            ls=s.split(':')\n",
    "            word=ls[0] #原词\n",
    "            ls[1]=ls[1][1:-1]\n",
    "            ls[1]=ls[1].split(' ')\n",
    "            sim_words=[eval(x) for x in ls[1]] #相似词构成的列表\n",
    "            self.words_similarity[word]=sim_words \n",
    "        return \n",
    "        \n",
    "    def process(self,x): #处理，去除标点，停用词和低频词\n",
    "        x=re.sub('[^A-Za-z]+', ' ', x).lower()\n",
    "        x = x.split(' ')\n",
    "        ls = [z for z in x if z not in self.stop_words]\n",
    "        cnt=Counter(ls)\n",
    "        res = [z for z in ls if cnt[z]>5]\n",
    "        return res\n",
    "    \n",
    "    def build_words_Dictionary(self):\n",
    "        vocab=set()\n",
    "        for pro in self.data['processed']:\n",
    "            vocab|=set(pro) \n",
    "        vocab=sorted(list(vocab))\n",
    "\n",
    "        wfile=open('./data/vocab.txt','w')\n",
    "        for word in vocab:\n",
    "            print(word,file=wfile)\n",
    "        wfile.close()\n",
    "        return vocab\n",
    "    \n",
    "    def IDF(self):\n",
    "        res={}\n",
    "        Y=len(self.data)\n",
    "        for word in self.vocab:\n",
    "            cnt=1\n",
    "            for processed_data in self.data['processed']:\n",
    "                if word in processed_data:\n",
    "                    cnt+=1\n",
    "            res[word]=math.log(Y/cnt)\n",
    "        return res\n",
    "\n",
    "    def TF_IDF_vec(self,data):\n",
    "        res=np.zeros(len(self.vocab))\n",
    "        cnt=Counter(data)\n",
    "        N=len(data)\n",
    "        for word in data:\n",
    "            loc=self.vocab.index(word)\n",
    "            tmp=self.IDF_dict[word]*cnt[word]/N\n",
    "            res[loc]=tmp\n",
    "        return res\n",
    "\n",
    "    \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        TODO：请在服务器端实现合理的并发处理方案，使得服务器端能够处理多个客户端发来的请求\n",
    "        \"\"\"\n",
    "        try:\n",
    "            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "            server.bind(self.address)\n",
    "            server.listen(5)\n",
    "        except socket.error as msg:\n",
    "            print(msg)\n",
    "            sys.exit(1)\n",
    "        print('Waiting connection...')\n",
    "\n",
    "        while 1:\n",
    "            conn,addr=server.accept()\n",
    "            print(f'{conn}connected!')\n",
    "            t=Thread(target=self.TextSearch,args=(conn,addr))\n",
    "            t.start()\n",
    "\n",
    "\n",
    "    def TextSearch(self,conn,addr):\n",
    "        \n",
    "        conn.send((\"Connected!\").encode())\n",
    "        text=conn.recv(1024).decode()\n",
    "        \n",
    "        #print(text)\n",
    "        \n",
    "        words_list=text.split(' ')\n",
    "        useful_data=np.array(self.data[['title','body','vec','id']])\n",
    "        res=[] #返回的检索内容\n",
    "        all_title=[] #记录全部标题，从而不会将相同标题的文章两次加到检索内容中\n",
    "        all_id=[]\n",
    "        all_vec=[]\n",
    "        new_words_list=[]\n",
    "        for word in words_list:\n",
    "            if word not in self.vocab:\n",
    "                continue\n",
    "            new_words_list.extend(self.words_similarity[word])#将原词的相似词也加到检索词的队列中\n",
    "\n",
    "        for word in new_words_list:\n",
    "            if word not in self.vocab:\n",
    "                continue\n",
    "            loc=self.vocab.index(word)\n",
    "            for data in useful_data:\n",
    "                tmp=data[2][loc]\n",
    "                if data[0] not in all_title and tmp!=0:\n",
    "                    res.append([data[0],data[1],tmp,data[3]-1]) #标题，正文，TF-IDF，id构成的元组，注意id减了1\n",
    "                    all_title.append(data[0])\n",
    "                    all_id.append(data[3]-1)\n",
    "                    all_vec.append(data[2])\n",
    "        \n",
    "        \n",
    "        '''for i,vec in enumerate(all_vec):\n",
    "            all_vec[i]=vec[all_id]'''\n",
    "        if all_vec!=[]:\n",
    "            useful_similirity=similarity(all_vec)\n",
    "            print(useful_similirity.shape)\n",
    "            for i,x in enumerate(res):\n",
    "                res[i][2]=sum(useful_similirity[i]) #计算该文章与其他检索到的文章的相似度的总和，并覆盖掉TF-IDF值(即x[2])\n",
    "                #print(res[i][2])\n",
    "\n",
    "            res=sorted(res,key=lambda x:x[2],reverse=True) #按照相似度总和的值降序排序\n",
    "            #print(res)\n",
    "            res=np.array(res,dtype=object) #将res先变成array，只取res的前两列（标题和正文），然后再返回\n",
    "            res=res[:,:2]\n",
    "            res=[tuple(x) for x in res]\n",
    "            \n",
    "            conn.send((repr(res)).encode())\n",
    "            conn.close()\n",
    "        else:\n",
    "            conn.send(('[]').encode())\n",
    "            conn.close()\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: 请补充实现文本检索，以及服务器端与客户端之间的通信\n",
    "        \n",
    "        1. 接受客户端传递的数据， 例如检索词\n",
    "        2. 调用检索函数，根据检索词完成检索\n",
    "        3. 将检索结果发送给客户端，具体的数据格式可以自己定义\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行服务器端\n",
    "启动服务器之后，在run.ipynb中运行客户端图形界面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "type(vec)== <class 'pandas.core.series.Series'>\n",
      "(1688, 2225)\n",
      "2225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/1688 [00:15<21:02,  1.32it/s]"
     ]
    }
   ],
   "source": [
    "server = LocalServer('127.0.0.1', 1234)\n",
    "server.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11004468  1.07810417 -0.01778058 -0.36007987]\n",
      " [-0.101603    0.67025721 -0.12684299 -1.45139029]\n",
      " [-0.08519805 -1.02044191 -0.21979702 -0.22826723]\n",
      " [ 0.8640683   0.97690409  0.52404301  0.54047315]\n",
      " [ 0.34574981 -0.16380384 -0.34532723  1.18995295]\n",
      " [ 0.56118092  0.78218456  2.19215272 -0.69094032]\n",
      " [-0.31619402  1.11948573 -1.85927889 -0.91258706]\n",
      " [ 0.25969365  0.48056369 -0.31125274  1.36486657]]\n",
      "-0.21979701569034235\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D'])\n",
    "#print (df.iloc[[1, 3, 5], [1, 3]])\n",
    "#print (df.iloc[1:3, :])\n",
    "#print (df.iloc[:,1:3])\n",
    "test=np.array(df)\n",
    "print(test)\n",
    "print(test[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 4)]\n",
      "[(1, 2), (3, 4)]\n",
      "[(1, 2), (3, 4)] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "ls=[(1,2),(3,4)]\n",
    "print(ls)\n",
    "print(repr(ls))\n",
    "s=repr(ls)\n",
    "s=eval(s)\n",
    "print(s,type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2, 3), (4, 5, 6), (7, 8, 9)]\n",
      "[(7, 8, 9), (4, 5, 6), (1, 2, 3)]\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 2]\n",
      " [4 5]\n",
      " [7 8]]\n",
      "[(1, 2), (4, 5), (7, 8)]\n"
     ]
    }
   ],
   "source": [
    "ls=[(1,2,3),(4,5,6),(7,8,9)]\n",
    "print(ls)\n",
    "print(sorted(ls,key=lambda x:x[2],reverse=True))\n",
    "ls=np.array(ls)\n",
    "print(ls)\n",
    "ls=ls[:,:2]\n",
    "print(ls)\n",
    "ls=[tuple(x) for x in ls]\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 3]\n",
      " [7 9]]\n",
      "10\n",
      "[[  1 100   3]\n",
      " [  4 100   6]\n",
      " [  7 100   9]] (3, 3)\n",
      "(3,)\n",
      "(3, 2) (2, 3)\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "[1 3 5]\n",
      "[2 4 6]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]] [[1, 2], [3, 4], [5, 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ls=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(ls)\n",
    "loc=[0,2]\n",
    "print(ls[loc][:,loc])\n",
    "print(sum(ls[1,loc]))\n",
    "for data in ls:\n",
    "    data[1]=100\n",
    "print(ls,ls.shape)\n",
    "print(ls[:,2].shape)\n",
    "ls2=np.array([[1,2],[3,4],[5,6]])\n",
    "print(ls2.shape,ls2.T.shape)\n",
    "print(ls2.T)\n",
    "for x in ls2.T:\n",
    "    print(x)\n",
    "print(ls2,[list(x) for x in ls2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d9d638e79dac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "ls=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "for i,l in enumerate(ls):\n",
    "    ls[i]=np.array(l)\n",
    "print(ls)\n",
    "print(np.array(ls))\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "new_add=np.vectorize(add)\n",
    "arr=np.array([1,2,3])\n",
    "print(new_add(ls,ls))\n",
    "print(new_add(arr,arr.T))\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [2, 3, 4]\n",
      "1    [5, 6, 7]\n",
      "dtype: object\n",
      "[array([2, 3, 4]) array([5, 6, 7])]\n",
      "[[2 5]\n",
      " [3 6]\n",
      " [4 7]]\n"
     ]
    }
   ],
   "source": [
    "ls=pd.Series([np.array([2,3,4]),np.array([5,6,7])])\n",
    "print(ls,np.array(ls).T,np.array([list(x) for x in ls]).T,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a' 'b' 'c'\n",
      "[\"'a'\", \"'b'\", \"'c'\"]\n",
      "['a', 'b', 'c']\n",
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "s=\"['a' 'b' 'c']\"\n",
    "s=s[1:-1]\n",
    "print(s)\n",
    "ls=s.split(' ')\n",
    "print(ls)\n",
    "print([eval(x) for x in ls])\n",
    "ls3=['a','b','c']\n",
    "print(ls3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "class myClass:\n",
    "    def __init__(self):\n",
    "        for i in tqdm(range(50)):\n",
    "            sleep(0.1)\n",
    "m=myClass()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
